--- simple_chat.py	2024-09-10 16:45:51
+++ simple_chat_with_history.py	2024-09-10 15:28:38
@@ -38,7 +38,7 @@
 
     st.sidebar.button("Clear conversation", key="clear_conversation")
     st.sidebar.toggle("Debug", key="debug", value=True)
-    st.sidebar.toggle("Use chat history", key="use_chat_history", value=True, disabled=True)
+    st.sidebar.toggle("Use chat history", key="use_chat_history", value=True, disabled=False)
 
     with st.sidebar.expander("Advanced options"):
         st.markdown("### Select model:")
@@ -47,7 +47,7 @@
         st.selectbox(
             "Aggregation of answers:", MODELS, key="model_name__aggregation", index=1, disabled=True
         )
-        st.selectbox("Summary of chat:", MODELS, key="model_name__summary", index=1, disabled=True)
+        st.selectbox("Summary of chat:", MODELS, key="model_name__summary", index=1, disabled=False)
 
         st.number_input(
             "Select number of context chunks",
@@ -69,6 +69,19 @@
     st.sidebar.expander("Session State").write(st.session_state)
 
 
+def get_chat_history() -> List[str]:
+    """
+    Retrieve the chat history from the session state limited
+    to the number of messages specified by the user
+    in the sidebar options.
+
+    Returns:
+        list: The list of chat messages from the session state.
+    """
+    start_index = max(0, len(st.session_state.messages) - st.session_state.num_chat_messages)
+    return st.session_state.messages[start_index : len(st.session_state.messages) - 1]
+
+
 def complete(model: str, prompt: str) -> str:
     """
     Generate a completion for the given prompt using the specified model.
@@ -83,6 +96,43 @@
     return session.sql("SELECT snowflake.cortex.complete(?,?)", (model, prompt)).collect()[0][0]
 
 
+def make_chat_history_summary(chat_history: str, question: str) -> str:
+    """
+    Generate a summary of the chat history combined with the current
+    question to extend the query
+    context. Use the language model to generate this summary.
+
+    Args:
+        chat_history (str): The chat history to include in the summary.
+        question (str): The current user question to extend with the chat history.
+
+    Returns:
+        str: The generated summary of the chat history and question.
+    """
+    prompt = dedent(
+        f"""
+        Based on the chat history below and the question, generate
+        a query that extend the question with the chat history provided.
+        The query should be in natural language.
+        Answer with only the query. Do not add any explanation.
+
+        <chat_history>
+        {chat_history}
+        </chat_history>
+        <question>
+        {question}
+        </question>
+    """
+    )
+
+    summary = complete(st.session_state.model_name__summary, prompt)
+
+    if st.session_state.debug:
+        st.sidebar.text_area("Chat history summary", summary.replace("$", "\$"), height=150)
+
+    return summary
+
+
 def create_generic_prompt(user_question: str):
     """
     Create a prompt for the language model using chat history (if enabled).
@@ -95,16 +145,25 @@
     Returns:
         str: The generated prompt for the language model.
     """
+    if st.session_state.use_chat_history:
+        chat_history = get_chat_history()
+    else:
+        chat_history = ""
 
     prompt = dedent(
         f"""
         You are a helpful AI chat assistant.
 
+        Use chat history provided between <chat_history>
+        and </chat_history> tags.
         Question is between <question> and </question> tags.
 
         If you don't know the answer just say: "I do not know the answer".
 
 
+        <chat_history>
+        {chat_history}
+        </chat_history>
         <question>
         {user_question}
         </question>
